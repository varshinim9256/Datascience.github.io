# -*- coding: utf-8 -*-
"""Varshini M.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oP96uCOvd-6_fQe7LXsFS3DRs2qz94FC
"""

#import necessary libraries
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
import numpy as np
from sklearn.preprocessing import StandardScaler
from numpy import exp
import warnings
warnings.filterwarnings("ignore")

data=pd.read_csv("/content/BankNote_Authentication.csv")

data.head()

data.tail()

data.info()

data["class"].unique

#data distribution
sns.pairplot(data, hue="class", height=3, diag_kind="kde")
plt.show(sns)

from sklearn.model_selection import train_test_split
train,test=train_test_split(data,shuffle=True)

x_train=[]
y_train=[]
for i in range(0,len(train)):
    x_train.append([train.values[i,0],train.values[i,1],train.values[i,2],train.values[i,3]])
    if train.values[i,4]== 0:
        y_train.append([0])
    else:
        y_train.append([1])

#scaling
sc = StandardScaler()
sc.fit(x_train)
xtrain=sc.transform(x_train)

fig, (ax1, ax2) = plt.subplots(ncols=2,figsize=(12,5))
ax1.set_title('Before Scaling')
ax2.set_title('After scaling')
print("\n\n\n\n\n\n\n\n   Input values before and after Scaling")
label_names=['varience','skewness','curtosis','entropy']
for i in range(0,4):
    t1=[]
    t2=[]
    for row1 in x_train:
        t1.append(row1[i])
    for row2 in xtrain:
        t2.append(row2[i])
    sns.kdeplot(t1,ax=ax1,label=label_names[i])
    sns.kdeplot(t2,ax=ax2,label=label_names[i])
plt.show()

class NeuralNet:
    def __init__(self,x,y,lr,epoch,k0,k1,b1,b2):
        np.random.seed(100)
        self.input      = x
        self.weights1   = np.random.rand(self.input.shape[1],6) 
        self.weights2   = np.random.rand(6,1)                 
        self.y          = y
        self.output     = np.zeros(self.y.shape)
        self.lr         = lr
        self.epoch      = epoch
        self.b1         = b1
        self.b2         = b2
        self.costlist   = []
        self.k0         = k0
        self.k1         = k1

    def softmax(self,x):
        e = exp(x)
        return e / e.sum()
    
    def derived_acf(self,x): 
        val = self.k0 +self.k1 * x  
        return val   

    def cost(self,y_target,y_output):
        return 0.5*np.sum(np.square(np.subtract(y_target,y_output)))
    
    def feedforward(self):
        self.layer1 = self.softmax((np.dot(self.input,self.weights1)+self.b1))
        self.output = self.derived_acf((np.dot(self.layer1,self.weights2)+self.b2))

    def backprop(self):
        d_weights2 = np.dot(self.layer1.T, ((self.y - self.output) * self.derived_acf(self.output)))
        d_weights1 = np.dot(self.input.T,(np.dot((self.y - self.output) * self.derived_acf(self.output), self.weights2.T) * self.softmax(self.layer1)))

        # update the weights with the derivative (slope) of the cost function
        self.weights1 += d_weights1 * self.lr
        self.weights2 += d_weights2 * self.lr 
        print("updated weights1 = ",self.weights1)
        print("updated weights2 = ",self.weights2)

    def train(self):
        for i in range (self.epoch):
          self.feedforward()
          self.backprop()
          self.costlist.append(self.cost(self.y,self.output))
            
    def predict(self,input_data):
        self.input=input_data
        self.feedforward()
        return self.output

if __name__ == "__main__":
  nn = NeuralNet(x=xtrain,y=np.array(y_train),lr=0.03,epoch=5,k0=0.5,k1=0.5,b1=1,b2=1)
  nn.train()
  nn.backprop()
  y_predtrain=[]
  for i in xtrain:
    ypretrain=nn.predict(i)
    if ypretrain.all()<=0.005:
      y_predtrain.append(0)
    else:
      y_predtrain.append(1)
    

  ##Plotting the cost vs epoch
  ep=[]
  for i in range (0,len(nn.costlist)):
      ep.append(i)
  plt.plot(ep,nn.costlist,marker = '^')
  plt.xlabel('epoch')
  plt.ylabel('Cost function')
  plt.title("Cost function vs epoch")
  plt.xlim((0,100))
  plt.show()

#testing
x_test=[]
y_test=[]
for i in range(0,len(test)):
    x_test.append([test.values[i,0],test.values[i,1],test.values[i,2],test.values[i,3]])
    if test.values[i,4]== 0 :
        y_test.append([0])
    else:
        y_test.append([1])

#scaling
sc = StandardScaler()
sc.fit(x_test)
xtest=sc.transform(x_test)

if __name__ == "__main__":
  nn = NeuralNet(x=xtest,y=np.array(y_test),lr=0.03,epoch=5,k0=0.5,k1=0.5,b1=1,b2=1)
  nn.train()
  nn.backprop()
  y_pred=[]
  for i in xtest:
    ypre=nn.predict(i)
    if ypre.all()<=0.005:
      y_pred.append(0)
    else:
      y_pred.append(1)
  
  ep=[]
  for i in range (0,len(nn.costlist)):
      ep.append(i)
  plt.plot(ep,nn.costlist,marker = '^')
  plt.xlabel('epoch')
  plt.ylabel('Cost function')
  plt.title("Cost function vs epoch")
  plt.xlim((0,100))
  plt.show()

from sklearn.metrics import classification_report,f1_score
print(classification_report(y_test,y_pred))

print(classification_report(y_train,y_predtrain))

